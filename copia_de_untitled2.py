# -*- coding: utf-8 -*-
"""Copia de Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVlK4YxxiILWzMWnh3pbxnlbIo4J__Cc

ANALISIS EXPLORATORIO DE DATOS EDA
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('punkt')
nltk.download('stopwords')

# Iniciamos explorando los primeros datos del dataset
datos = pd.read_csv('Nuevo_Dataset_Patrones_Emocionales.csv')
datos.head()

print("\nInformación del DataFrame:")
datos.info()

print("\nValores nulos por columna:")
print(datos.isnull().sum())

print("--- Nombres de las columnas ---")
print(datos.columns)

num_filas = datos.shape[0]
print(f"El dataset tiene {num_filas} filas.")

"""2.preprocesamiento de texto"""

# Imputamos filas con valores nulos
datos = datos.dropna()
# Contamos si hay valores nulos despues de imputar
print(datos.isnull().sum())
print (f"Filas después de eliminar valores faltantes: {datos.shape[0]}")

# Checamos los valores nulos
datos.isnull().sum()

"""PREPROCESAMIENTO DEL TEXTO

limpieza de texto
"""

import re

def limpiar_texto(texto: str) -> str:
    if not isinstance(texto, str):
        return ""
    texto = texto.lower()
    texto = re.sub(r'[^a-z\s]', '', texto)
    return texto

# Seleccionar columnas de texto desde la columna 12 en adelante
text = datos.iloc[:, 0:]

# Aplicar la limpieza a todas las columnas de texto
for column in text.columns:
    text[column] = text[column].apply(limpiar_texto)

# Ver resultado
print(text.head())

"""Tokenizacion"""

from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
def tokenizar(cell):
    if isinstance(cell, list):
        return ' '.join(cell)
    elif pd.isnull(cell): #checar valores NaN
        return '' #si el valor es nulo pasarlo a texto vacio
    else:
        return cell

for column in text.columns:
  text[column] = text[column].apply(word_tokenize)

#Tokenizar los textos
print(text.head())

"""eliminacion de stopwords"""

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
#Cargar stopwords en españo
stop_words = set(stopwords.words('spanish'))
#Elimina los stopwords
for column in text.columns:
  text[column] = text[column].apply(lambda x: [word for word in x if word not in stop_words])

print(text.head())

"""lematizacion"""

# Para poder cargar el modelo en español
!python -m spacy download es_core_news_sm

import spacy
import pandas as pd

# Cargar el modelo en español de spacy
nlp = spacy.load('es_core_news_sm')


# Función para lematizar texto en español
def lemmatizar(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])
for column in text.columns:
    text[column] = text[column].apply(lambda x: [lemmatizar(word) for word in x])

print(text.head())

from wordcloud import WordCloud

#Generar la nube de palabras
wordcloud = WordCloud(width=800, height=400).generate(''.join(text.astype(str).apply(''.join, axis=1).values))

#Mostrar la nube de palabras
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

def emocion_para_columna(col):
    pregunta_numero = int(col.split('.')[0])  # Extraer el número antes del punto
    if pregunta_numero == 1:
        return 'Felicidad'
    elif pregunta_numero == 3:
        return 'Tristeza'
    elif pregunta_numero == 5:
        return 'Disgusto'
    elif pregunta_numero == 7:
        return 'Ira'
    elif pregunta_numero == 9:
        return 'Miedo'
    elif pregunta_numero == 11:
        return 'Sorpresa'
    return None

# Filtrar las columnas que son preguntas
preguntas_reales = [col for col in text.columns if col.strip()[0].isdigit()]

# Crear un nuevo DataFrame donde asociamos respuestas con sentimientos
data_respuestas_con_emociones = pd.DataFrame()

for pregunta in preguntas_reales:
    emocion = emocion_para_columna(pregunta)
    temp_df = pd.DataFrame({
        'Pregunta': [pregunta] * len(text),  # Columna con la pregunta
        'Respuesta': text[pregunta],         # Respuestas correspondientes
        'Emocion': [emocion] * len(text)  # La emocion asociada a la pregunta
    })
    data_respuestas_con_emociones = pd.concat([data_respuestas_con_emociones, temp_df], ignore_index=True)

# Mostrar las primeras filas del nuevo dataset
data_respuestas_con_emociones.head()

# @title Emocion

from matplotlib import pyplot as plt
import seaborn as sns
data_respuestas_con_emociones.groupby('Emocion').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""nube de palabras por emocion"""

# prompt: Generar nubes de palabras para cada categoría de emoción,
# con el fin de identificar patrones o palabras clave comunes que puedan caracterizar cada
# una de las emociones.

for categoria, data_categoria in data_respuestas_con_emociones.groupby('Emocion'):
    # Combinar todos los textos de respuesta para esta categoría de emoción
    texto_combinado = ' '.join([' '.join(respuesta) for respuesta in data_categoria['Respuesta']])

    # Generar la nube de palabras
    wordcloud = WordCloud(width=800, height=400, background_color='black').generate(texto_combinado)

    # Mostrar la nube de palabras
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Nube de palabras para la emoción: {categoria}')
    plt.show()

print(data_respuestas_con_emociones.head)

import pandas as pd

# 1. Filtrar las columnas que son preguntas (empiezan con dígito)
preguntas_reales_emb = [col for col in datos.columns if col.strip()[0].isdigit()]

# 2. Hacer un melt para convertir columnas en filas
df_long = (
    datos[preguntas_reales_emb]
    .reset_index(drop=True)  # opcional, para tener un índice limpio
    .melt(
        var_name='Pregunta',
        value_name='Respuesta'
    )
)

# 3. Asignar el sentimiento usando tu función
#    Suponemos que ya tienes definida:
#    def asignar_sentimiento_para_columna(col): ...

df_long['Emocion'] = df_long['Pregunta'].apply(emocion_para_columna)

# 4. Reordenar/seleccionar las columnas finales
data_respuestas_con_sentimientos_emb = df_long[['Pregunta', 'Respuesta', 'Emocion']]

# 5. Ver las primeras filas
print(data_respuestas_con_sentimientos_emb.head())

"""vectorizacion"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd # Asegurarse de importar pandas

# La función emocion_para_columna ya está definida previamente.

# Filtrar las columnas que son preguntas (empiezan con dígito)
# Usamos el DataFrame 'text' que ya contiene el texto preprocesado
preguntas_reales_emb = [col for col in text.columns if col.strip()[0].isdigit()]

# Hacer un melt para convertir columnas en filas usando el DataFrame 'text'
df_long_processed = (
    text[preguntas_reales_emb]
    .reset_index(drop=True)  # opcional, para tener un índice limpio
    .melt(
        var_name='Pregunta',
        value_name='Respuesta_Preprocesada' # Renombrar para claridad
    )
)

# Asignar la emoción usando tu función
df_long_processed['Emocion'] = df_long_processed['Pregunta'].apply(emocion_para_columna)

# Reordenar/seleccionar las columnas finales
# Creamos el DataFrame para la vectorización
data_respuestas_con_sentimientos_emb = df_long_processed[['Pregunta', 'Respuesta_Preprocesada', 'Emocion']].copy() # Usar .copy() para evitar SettingWithCopyWarning

# Unir las listas de palabras preprocesadas en una sola cadena para TF-IDF
# Asegurarse de manejar casos donde la lista pueda estar vacía (aunque stopword removal debería dejar algo)
data_respuestas_con_sentimientos_emb['clean_text'] = data_respuestas_con_sentimientos_emb['Respuesta_Preprocesada'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')


# Vectorizar con TF-IDF
tfidf_vectorizer = TfidfVectorizer()
# Aplicar fit_transform al texto limpio y unido
X_tfidf = tfidf_vectorizer.fit_transform(data_respuestas_con_sentimientos_emb['clean_text'])

# Mostrar las primeras características (si existen)
if len(tfidf_vectorizer.get_feature_names_out()) > 0:
    print(tfidf_vectorizer.get_feature_names_out()[:1000])
else:
    print("El vocabulario está vacío después de la vectorización. Revisa el preprocesamiento y los datos.")

# Mostrar las primeras filas del nuevo dataset para verificar
print(data_respuestas_con_sentimientos_emb.head())

from wordcloud import WordCloud

#Generar la nube de palabras
wordcloud = WordCloud(width=800, height=400).generate(''.join(text.astype(str).apply(''.join, axis=1).values))

#Mostrar la nube de palabras
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import pandas as pd

# Se analiza cuantas respuestas hay por emoción

df = data_respuestas_con_emociones.copy()

# Cantidad de respuestas que hay por cada pregunta
cuentas_por_pregunta = df.groupby('Pregunta').size().sort_index()
print("Respuestas por pregunta:")
print(cuentas_por_pregunta, "\n")

# Cantidad de respuestas hay por cada emoción
cuentas_por_emocion = df.groupby('Emocion').size()
print("Respuestas por emoción:")
print(cuentas_por_emocion, "\n")

# Tabla cruzada pregunta vs emoción
tabla_pregunta_emocion = (
    df
    .groupby(['Pregunta', 'Emocion'])
    .size()
    .unstack(fill_value=0)
    .sort_index()
)
print("Matriz Pregunta x Emoción:")
print(tabla_pregunta_emocion)

import matplotlib.pyplot as plt

# Contar cuántas respuestas hay de cada emoción
cuentas_emociones = data_respuestas_con_emociones['Emocion'].value_counts()

# Gráfico de barras
plt.figure(figsize=(8, 5))
cuentas_emociones.plot(kind='bar')
plt.title("Distribución de Emociones")
plt.xlabel("Emoción")
plt.ylabel("Número de respuestas")
plt.tight_layout()
plt.show()

"""GENERACION DE EMBEDDINGS USANDO TRANSFORMERS"""

import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
from tqdm import tqdm
import pickle

def generar_embeddings_transformer(df, columna_texto='Respuesta', modelo='dccuchile/bert-base-spanish-wwm-cased',
                                 batch_size=16, max_length=256, output_file='embeddings_transformer.pkl'):

    # Verificar que la columna de texto exista en el DataFrame
    if columna_texto not in df.columns:
        raise ValueError(f"La columna '{columna_texto}' no existe en el DataFrame")

    # Extraer textos del DataFrame
    textos = df[columna_texto].tolist()

    # Verificar si los textos están en forma de lista (como en tu ejemplo)
    # y convertirlos a cadenas de texto si es necesario
    if isinstance(textos[0], list):
        print("Detectado: los textos están en formato de lista, convirtiendo a cadenas...")
        textos = [' '.join(map(str, texto)) for texto in textos]

    print(f"Procesando {len(textos)} textos...")

    # Cargar tokenizer y modelo
    print(f"Cargando modelo '{modelo}'...")
    tokenizer = AutoTokenizer.from_pretrained(modelo)
    model = AutoModel.from_pretrained(modelo)

    # Verificar si hay GPU disponible
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Utilizando dispositivo: {device}")
    model = model.to(device)
    model.eval()

    # Lista para almacenar todos los embeddings
    all_embeddings = []

    # Procesar textos en batches
    for i in tqdm(range(0, len(textos), batch_size), desc="Generando embeddings"):
        batch_texts = textos[i:i+batch_size]

        # Tokenizar textos
        encoded_inputs = tokenizer(
            batch_texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors='pt'
        ).to(device)

        # Obtener embeddings
        with torch.no_grad():
            outputs = model(**encoded_inputs)

        # Extraer las representaciones de la última capa del transformer
        # Opción 1: Usar el token [CLS] como representación del texto completo
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

        # Alternativa: Promediar todos los tokens (mean pooling)
        # attention_mask = encoded_inputs['attention_mask'].cpu().numpy()
        # batch_embeddings = (outputs.last_hidden_state.cpu().numpy() * attention_mask[:, :, None]).sum(axis=1) / attention_mask.sum(axis=1)[:, None]

        all_embeddings.append(batch_embeddings)

    # Concatenar todos los embeddings
    embeddings = np.vstack(all_embeddings)

    # Normalizar los embeddings (L2)
    print("Normalizando embeddings...")
    embeddings = normalize(embeddings, norm='l2')

    print(f"Forma final de los embeddings: {embeddings.shape}")

    # Guardar embeddings y metadatos
    print(f"Guardando embeddings en '{output_file}'...")
    with open(output_file, 'wb') as f:
        pickle.dump({
            'embeddings': embeddings,
            'ids': df.index.tolist(),
            'etiquetas': df['Emocion'].tolist() if 'Emocion' in df.columns else None,
            'modelo': modelo,
            'dimension': embeddings.shape[1]
        }, f)

    print("¡Embeddings generados y guardados con éxito!")
    return embeddings

if __name__ == "__main__":
    # Usa directamente la variable 'datos' que ya tienes en memoria
    print("Usando variable 'datos' ya en memoria...")
    data = data_respuestas_con_emociones  # Usa la variable datos que ya tienes cargada
    print(f"Datos accedidos con éxito. Shape: {data.shape}")

    # Configuración para generar embeddings
    columna_texto = 'Respuesta'  # Ajustado a la columna que contiene los textos
    modelo_transformer = 'dccuchile/bert-base-spanish-wwm-cased'  # Modelo para español

    # Alternativas de modelos en español:
    # modelo_transformer = 'bertin-project/bertin-roberta-base-spanish'  # RoBERTa para español
    # modelo_transformer = 'hackathon-pln-es/roberta-base-bne'  # RoBERTa entrenado en corpus español BNE
    # modelo_transformer = 'PlanTL-GOB-ES/roberta-base-bne'  # Versión actualizada de RoBERTa BNE

    # Generar embeddings
    embeddings = generar_embeddings_transformer(
        df=data,
        columna_texto=columna_texto,
        modelo=modelo_transformer,
        batch_size=16,  # Ajusta según tu memoria disponible
        max_length=256,  # Ajusta según la longitud promedio de tus textos
        output_file='embeddings_transformer.pkl'
    )

    # Mostrar información sobre los embeddings generados
    print(f"\nResumen de embeddings generados:")
    print(f"- Número de documentos: {embeddings.shape[0]}")
    print(f"- Dimensiones por documento: {embeddings.shape[1]}")
    print(f"- Modelo utilizado: {modelo_transformer}")

"""DIVISION DEL DATASET"""

import pickle

# Cargar y mostrar matriz
with open('embeddings_transformer.pkl', 'rb') as f:
    matriz = pickle.load(f)['embeddings']

# Ver dimensiones y muestra
print(f"Forma: {matriz.shape}")
print(matriz[:3, :5])  # Primeras 3 filas, primeras 5 columnas

y = data_respuestas_con_emociones['Emocion']
x = X_tfidf
#Dividir en entrenamiento (80%) y prueba (20%)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print(f"Datos de entrenamiento: {x_train.shape}")
print(f"Datos de prueba: {x_test.shape}")

"""IMPLEMENTACION DE LOS ALGORTIMOS


"""

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split # Asegurarse de que esté importado

def entrenar_y_evaluar_svm(X_train, X_test, y_train, y_test, modelo_nombre, kernel):
    clf_svm = SVC(kernel=kernel)
    clf_svm.fit(X_train, y_train)
    accuracy = clf_svm.score(X_test, y_test)
    print(f"Precisión(Accuracy) con {modelo_nombre} usando SVM ({kernel} kernel): {accuracy:.2f}")
    precision = precision_score(y_test, clf_svm.predict(X_test), average='weighted')
    recall = recall_score(y_test, clf_svm.predict(X_test), average='weighted')
    f1 = f1_score(y_test, clf_svm.predict(X_test), average='weighted')
    print(f"Exacitud: {accuracy:.2f}")
    print(f"Precisión: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    return clf_svm

# Asumiendo que 'matriz' contiene los embeddings generados previamente
# Realizar la división en entrenamiento y prueba para los embeddings
x_emb_train, x_emb_test, y_train_emb, y_test_emb = train_test_split(matriz, y, test_size=0.2, random_state=42)

# Nota: Usamos y_train y y_test originales para los modelos no-embedding
# y_train_emb, y_test_emb deberían ser idénticos a y_train, y_test si los splits usan el mismo random_state.
# Para evitar confusiones, puedes usar y_train y y_test para ambos conjuntos (TF-IDF y Embeddings)
# ya que la división se basa en el índice de las filas, que es el mismo para ambos.

print(f"Datos de entrenamiento de embeddings: {x_emb_train.shape}")
print(f"Datos de prueba de embeddings: {x_emb_test.shape}")

# Entrenar y evaluar SVM con BERT (usando los embeddings)
clf_svm_bert_linear = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "BERT", 'linear')
clf_svm_bert_rbf = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "BERT", 'rbf')
clf_svm_bert_poly = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "BERT", 'poly')
clf_svm_bert_sigmoid = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "BERT", 'sigmoid')

# Entrenar y evaluar SVM con DistilBERT
# NOTA: Tu código original usa x_emb_train/test para BERT y DistilBERT.
# Si generaste embeddings de DistilBERT en algún paso anterior (no visible en el código proporcionado)
# deberías tener variables separadas (ej: x_distilbert_train, x_distilbert_test)
# para usar aquí. Si solo generaste embeddings de BERT (lo que parece por 'matriz'),
# estos resultados serán con los embeddings de BERT.
# Por ahora, mantendremos la estructura original pero es importante notar esto.
clf_svm_distilbert_linear = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "DistilBERT", 'linear')
clf_svm_distilbert_rbf = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "DistilBERT", 'rbf')
clf_svm_distilbert_poly = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "DistilBERT", 'poly')
clf_svm_distilbert_sigmoid = entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, "DistilBERT", 'sigmoid')


# Entrenar y evaluar SVM sin embeddings (usando TF-IDF)
clf_svm_linear = entrenar_y_evaluar_svm(x_train, x_test, y_train, y_test, "No embedding", 'linear')
clf_svm_rbf = entrenar_y_evaluar_svm(x_train, x_test, y_train, y_test, "No embedding", 'rbf')
clf_svm_poly = entrenar_y_evaluar_svm(x_train, x_test, y_train, y_test, "No embedding", 'poly')
clf_svm_sigmoid = entrenar_y_evaluar_svm(x_train, x_test, y_train, y_test, "No embedding", 'sigmoid')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder # Import LabelEncoder
import pandas as pd # Import pandas

clf_svm = SVC(kernel='linear')
clf_svm.fit(x_train, y_train)
# Predecir valores
y_pred = clf_svm.predict(x_test)

# Calcular la matriz de confusión
cm = confusion_matrix(y_test, y_pred)

# Crear y ajustar el LabelEncoder para obtener las clases
le = LabelEncoder()
# Fit on the combined train and test labels to ensure all classes are included
# Use pd.concat() instead of the invalid .append() method for Series
le.fit(pd.concat([y_train, y_test]))


# Visualizar la matriz de confusión
plt.figure(figsize=(12, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

def entrenar_y_evaluar_svm_kfold(X, y, modelo_nombre, kernel):
    clf_svm = SVC(kernel=kernel)
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    scores = cross_val_score(clf_svm, X, y, cv=kf)
    print(f"Precisión promedio con {modelo_nombre} usando SVM ({kernel} kernel): {scores.mean():.2f}")

    return scores

# Evaluar SVM con BERT usando K-Folds
scores_svm_bert_linear = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "BERT", 'linear')
scores_svm_bert_rbf = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "BERT", 'rbf')
scores_svm_bert_poly = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "BERT", 'poly')
scores_svm_bert_sigmoid = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "BERT", 'sigmoid')


# Evaluar SVM con DistilBERT usando K-Folds
scores_svm_distilbert_linear = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "DistilBERT", 'linear')
scores_svm_distilbert_rbf = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "DistilBERT", 'rbf')
scores_svm_distilbert_poly = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "DistilBERT", 'poly')
scores_svm_distilbert_sigmoid = entrenar_y_evaluar_svm_kfold(x_emb_train, y_train, "DistilBERT", 'sigmoid')

# Aquí usamos los datos completos TF-IDF (x) y las etiquetas (y)
scores_svm_linear = entrenar_y_evaluar_svm_kfold(x, y, "No embedding", 'linear')
scores_svm_rbf = entrenar_y_evaluar_svm_kfold(x, y, "No embedding", 'rbf')
scores_svm_poly = entrenar_y_evaluar_svm_kfold(x, y, "No embedding", 'poly')
scores_svm_sigmoid = entrenar_y_evaluar_svm_kfold(x, y, "No embedding", 'sigmoid')

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_score, recall_score, f1_score # Import necessary metrics

def entrenar_y_evaluar_arbol(X_train, X_test, y_train, y_test, modelo_nombre):
    clf_arbol = DecisionTreeClassifier(max_depth=10, random_state=42)  # Puedes ajustar `max_depth`
    clf_arbol.fit(X_train, y_train)
    accuracy = clf_arbol.score(X_test, y_test)
    print(f"Precisión (Accuracy) con {modelo_nombre} usando Árbol de Decisión: {accuracy:.2f}")
    precision = precision_score(y_test, clf_arbol.predict(X_test), average='weighted')
    recall = recall_score(y_test, clf_arbol.predict(X_test), average='weighted')
    f1 = f1_score(y_test, clf_arbol.predict(X_test), average='weighted')
    print(f"Exacitud: {accuracy:.2f}")
    print(f"Precisión: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    return clf_arbol

# Entrenar y evaluar Árboles de Decisión con BERT
clf_arbol_bert = entrenar_y_evaluar_arbol(x_emb_train, x_emb_test, y_train, y_test, "BERT")

# Entrenar y evaluar Árboles de Decisión con DistilBERT
# Corrección: Pasar x_emb_test como segundo argumento en lugar de y_train
clf_arbol_distilbert = entrenar_y_evaluar_arbol(x_emb_train, x_emb_test, y_train, y_test, "DistilBERT")

# Entrenar y evaluar Árboles de Decisión
clf_arbol_tfidf = entrenar_y_evaluar_arbol(x_train, x_test, y_train, y_test, "No embedding")

def entrenar_y_evaluar_arbol_kfold(X, y, modelo_nombre):
    clf_arbol = DecisionTreeClassifier(max_depth=10, random_state=42)
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    scores = cross_val_score(clf_arbol, X, y, cv=kf)
    print(f"Precisión promedio con {modelo_nombre} usando Árbol de Decisión: {scores.mean():.2f}")
    return scores

# Evaluar Árbol de Decisión con BERT usando K-Folds
scores_arbol_bert = entrenar_y_evaluar_arbol_kfold(x_emb_train, y_train, "BERT")

# Evaluar Árbol de Decisión con DistilBERT usando K-Folds
scores_arbol_distilbert = entrenar_y_evaluar_arbol_kfold(x_emb_train, y_train, "DistilBERT")

scores_arbol = entrenar_y_evaluar_arbol_kfold(x, y, "No embedding")

# Tabla para resumir los resultados
resultados = {
    "Modelo": [
               "SVM (BERT)", "SVM (DistilBERT)","SVM",
               "Árbol de Decisión (BERT)", "Árbol de Decisión (DistilBERT)","Arbol de Decision C4.5"],
    "Precisión": [

        scores_svm_bert_linear.mean(),
        scores_svm_distilbert_linear.mean(),
        scores_svm_linear.mean(),
        scores_arbol_bert.mean(),
        scores_arbol_distilbert.mean(),
        scores_arbol.mean()
    ]
}

resultados_df = pd.DataFrame(resultados)
print(resultados_df)

# Visualizar resultados en un gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(resultados_df["Modelo"], resultados_df["Precisión"], color=['blue', 'green', 'orange', 'red', 'purple', 'cyan'])
plt.xticks(rotation=45, ha='right')
plt.ylabel("Precisión")
plt.title("Comparación de Modelos")
plt.show()

# Crear un DataFrame con los resultados para el mapa de calor
resultados_heatmap = pd.DataFrame({
    "BERT": [

        scores_svm_bert_linear.mean(),
        scores_arbol_bert.mean()
    ],
    "DistilBERT": [

        scores_svm_distilbert_linear.mean(),
        scores_arbol_distilbert.mean()
    ],
    "No embedding": [

        scores_svm_linear.mean(),
        scores_arbol.mean()
    ]
}, index=[ "SVM", "Árbol de Decisión"])

# Configuración del mapa de calor
plt.figure(figsize=(8, 6))
sns.heatmap(resultados_heatmap, annot=True, cmap="YlGnBu", fmt=".2f", cbar=True)
plt.title("Precisión de Modelos por Tipo de Embedding")
plt.ylabel("Modelo")
plt.xlabel("Embeddings")
plt.show()

from sklearn.manifold import TSNE
import seaborn as sns

# Reducir dimensionalidad con t-SNE
def reducir_dimensionalidad_tsne(embeddings, etiquetas, modelo_nombre):
    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
    embeddings_2d = tsne.fit_transform(embeddings)

    # Crear un DataFrame para visualizar
    df_tsne = pd.DataFrame({
        'Dim1': embeddings_2d[:, 0],
        'Dim2': embeddings_2d[:, 1],
        'Etiqueta': etiquetas
    })

    plt.figure(figsize=(10, 8))
    sns.scatterplot(data=df_tsne, x="Dim1", y="Dim2", hue="Etiqueta", palette="tab10", s=50)
    plt.title(f"Visualización de Embeddings de {modelo_nombre} con t-SNE")
    plt.xlabel("Dimensión 1")
    plt.ylabel("Dimensión 2")
    plt.legend(title="emocion", loc='best', bbox_to_anchor=(1.05, 1))
    plt.show()

# Visualizar embeddings de BERT
reducir_dimensionalidad_tsne(x_emb_train, y_train, "BERT")

# Visualizar embeddings de DistilBERT
reducir_dimensionalidad_tsne(x_emb_train, y_train, "DistilBERT")

from sklearn.decomposition import PCA

# Reducir dimensionalidad con PCA
def reducir_dimensionalidad_pca(embeddings, etiquetas, modelo_nombre):
    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(embeddings)

    # Crear un DataFrame para visualizar
    df_pca = pd.DataFrame({
        'Dim1': embeddings_2d[:, 0],
        'Dim2': embeddings_2d[:, 1],
        'Etiqueta': etiquetas
    })

    plt.figure(figsize=(10, 8))
    sns.scatterplot(data=df_pca, x="Dim1", y="Dim2", hue="Etiqueta", palette="tab10", s=50)
    plt.title(f"Visualización de Embeddings de {modelo_nombre} con PCA")
    plt.xlabel("Dimensión 1 (Varianza: {:.2f}%)".format(pca.explained_variance_ratio_[0] * 100))
    plt.ylabel("Dimensión 2 (Varianza: {:.2f}%)".format(pca.explained_variance_ratio_[1] * 100))
    plt.legend(title="Sentimiento", loc='best', bbox_to_anchor=(1.05, 1))
    plt.show()
    return df_pca

# Visualizar embeddings de BERT
reducir_dimensionalidad_pca(x_emb_train, y_train, "BERT")

# Visualizar embeddings de DistilBERT
reducir_dimensionalidad_pca(x_emb_train, y_train, "DistilBERT")

#texto_ejemplo = "Me siento muy feliz cuando estoy con mi familia y amigos."
#texto_ejemplo = "me sorprendio ver tu coche"
#texto_ejemplo = "mi perrito se murio y me siento triste"
#texto_ejemplo = "me dan asco las ranas"
#texto_ejemplo = "me dan miedo los fantasmas"
#texto_ejemplo = "senti una ira al verlo de nuevo"
texto_ejemplo = "senti una ira inmensa al verlo"

# Vectorizar el texto de ejemplo usando el vectorizador TF-IDF entrenado
texto_ejemplo_vectorizado = tfidf_vectorizer.transform([texto_ejemplo])

# Realizar la predicción usando el modelo SVM entrenado con TF-IDF
# Asegúrate de que clf_svm es el modelo que deseas usar para predecir textos vectorizados con TF-IDF
prediccion = clf_svm.predict(texto_ejemplo_vectorizado)

# La predicción ya es el emocion predicho en formato string
emocion_predicho = prediccion[0] # Acceder al primer elemento del array de predicciones

print(f"Texto de ejemplo: {texto_ejemplo}")
print(f"emocion predicho: {emocion_predicho}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import pandas as pd # Import pandas


def probar_modelo(texto_ejemplo):
    # Vectorizar el texto de ejemplo
    texto_ejemplo_vectorizado = tfidf_vectorizer.transform([texto_ejemplo])

    # Predecir el resultado usando el modelo clf_svm
    # clf_svm.predict() returns the predicted class labels directly as strings
    prediccion = clf_svm.predict(texto_ejemplo_vectorizado)

    # The prediction is already the emotion predicted in string format
    # Access the first element of the prediction array
    emocion_predicho = prediccion[0]

    print(f"Texto de ejemplo: {texto_ejemplo}")
    print(f"emocion predicho: {emocion_predicho}")

probar_modelo("Me siento mal por ella.")

texto = input()
probar_modelo(texto)

"""KNN"""

# prompt: generame un codigo K-Nearest Neighbors (KNN): Experimentar con distintos valores de vecinos (k)
# para observar su impacto en la precisión del modelo y que tenga estas metricas de rendimiento: precision,recall,f1score,ROC-ACU, MATRYZ DE CONFUSION Y VALIDACION CRUZADA

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer

# Asegurarse de que las variables x (TF-IDF), y (etiquetas), x_emb_train y x_emb_test (embeddings)
# están disponibles en este punto del código.

# Convertir las etiquetas a formato binario para ROC-AUC (para clasificación multiclase)
# Se necesita un enfoque One-vs-Rest o similar para calcular ROC-AUC en multiclase.
# LabelBinarizer ayuda a crear la representación binaria de las etiquetas.
lb = LabelBinarizer()
y_train_bin = lb.fit_transform(y_train)
y_test_bin = lb.transform(y_test)

# Definir un rango de valores para k
k_values = range(1, 21, 2)  # Experimentar con k = 1, 3, 5, ..., 19

results = []

print("Experimentando con K-Nearest Neighbors (KNN):")

for k in k_values:
    print(f"\nEntrenando y evaluando KNN con k = {k}")

    # Inicializar el modelo KNN
    knn = KNeighborsClassifier(n_neighbors=k)

    # --- Entrenamiento y Evaluación en Datos de Prueba (TF-IDF) ---
    print("--- Usando TF-IDF ---")
    knn_tfidf = KNeighborsClassifier(n_neighbors=k)
    knn_tfidf.fit(x_train, y_train)
    y_pred_tfidf = knn_tfidf.predict(x_test)

    # Métricas de rendimiento para TF-IDF
    accuracy_tfidf = knn_tfidf.score(x_test, y_test)
    precision_tfidf = precision_score(y_test, y_pred_tfidf, average='weighted')
    recall_tfidf = recall_score(y_test, y_pred_tfidf, average='weighted')
    f1_tfidf = f1_score(y_test, y_pred_tfidf, average='weighted')

    # ROC-AUC (requiere probabilidades y formato binario)
    try:
        y_proba_tfidf = knn_tfidf.predict_proba(x_test)
        roc_auc_tfidf = roc_auc_score(y_test_bin, y_proba_tfidf, average='weighted')
    except Exception as e:
        roc_auc_tfidf = "N/A"
        print(f"No se pudo calcular ROC-AUC para TF-IDF: {e}")


    print(f"Precisión (Accuracy) TF-IDF (k={k}): {accuracy_tfidf:.4f}")
    print(f"Precisión TF-IDF (k={k}): {precision_tfidf:.4f}")
    print(f"Recall TF-IDF (k={k}): {recall_tfidf:.4f}")
    print(f"F1 Score TF-IDF (k={k}): {f1_tfidf:.4f}")
    print(f"ROC-AUC TF-IDF (k={k}): {roc_auc_tfidf}")

    # Matriz de Confusión para TF-IDF
    cm_tfidf = confusion_matrix(y_test, y_pred_tfidf)
    print(f"Matriz de Confusión TF-IDF (k={k}):\n{cm_tfidf}")

    # Validación Cruzada para TF-IDF
    kf = KFold(n_splits=5, shuffle=True, random_state=42) # Usar 5 splits
    cv_scores_tfidf = cross_val_score(knn_tfidf, x, y, cv=kf, scoring='accuracy') # Usar datos completos para CV
    print(f"Validación Cruzada (Accuracy) TF-IDF (k={k}): {cv_scores_tfidf.mean():.4f} (+/- {cv_scores_tfidf.std() * 2:.4f})")


    # --- Entrenamiento y Evaluación en Datos de Prueba (Embeddings) ---
    print("--- Usando Embeddings (BERT) ---")
    knn_emb = KNeighborsClassifier(n_neighbors=k)
    # Asegurarse de que x_emb_train y y_train_emb existen y tienen las dimensiones correctas
    # Si usaste el mismo split para TF-IDF y Embeddings, y_train_emb == y_train
    knn_emb.fit(x_emb_train, y_train)
    # Asegurarse de que x_emb_test existe
    y_pred_emb = knn_emb.predict(x_emb_test)

    # Métricas de rendimiento para Embeddings
    accuracy_emb = knn_emb.score(x_emb_test, y_test) # Usar y_test si el split fue el mismo
    precision_emb = precision_score(y_test, y_pred_emb, average='weighted')
    recall_emb = recall_score(y_test, y_pred_emb, average='weighted')
    f1_emb = f1_score(y_test, y_pred_emb, average='weighted')

     # ROC-AUC (requiere probabilidades y formato binario)
    try:
        y_proba_emb = knn_emb.predict_proba(x_emb_test)
        roc_auc_emb = roc_auc_score(y_test_bin, y_proba_emb, average='weighted')
    except Exception as e:
        roc_auc_emb = "N/A"
        print(f"No se pudo calcular ROC-AUC para Embeddings: {e}")

    print(f"Precisión (Accuracy) Embeddings (k={k}): {accuracy_emb:.4f}")
    print(f"Precisión Embeddings (k={k}): {precision_emb:.4f}")
    print(f"Recall Embeddings (k={k}): {recall_emb:.4f}")
    print(f"F1 Score Embeddings (k={k}): {f1_emb:.4f}")
    print(f"ROC-AUC Embeddings (k={k}): {roc_auc_emb}")


    # Matriz de Confusión para Embeddings
    cm_emb = confusion_matrix(y_test, y_pred_emb)
    print(f"Matriz de Confusión Embeddings (k={k}):\n{cm_emb}")


    # Validación Cruzada para Embeddings
    # Usar los embeddings completos (matriz) y etiquetas completas (y) para CV
    cv_scores_emb = cross_val_score(knn_emb, matriz, y, cv=kf, scoring='accuracy')
    print(f"Validación Cruzada (Accuracy) Embeddings (k={k}): {cv_scores_emb.mean():.4f} (+/- {cv_scores_emb.std() * 2:.4f})")


    # Almacenar resultados
    results.append({
        'k': k,
        'Accuracy_TFIDF': accuracy_tfidf,
        'Precision_TFIDF': precision_tfidf,
        'Recall_TFIDF': recall_tfidf,
        'F1_TFIDF': f1_tfidf,
        'ROC_AUC_TFIDF': roc_auc_tfidf,
        'CV_Accuracy_TFIDF': cv_scores_tfidf.mean(),
        'Accuracy_Embeddings': accuracy_emb,
        'Precision_Embeddings': precision_emb,
        'Recall_Embeddings': recall_emb,
        'F1_Embeddings': f1_emb,
        'ROC_AUC_Embeddings': roc_auc_emb,
        'CV_Accuracy_Embeddings': cv_scores_emb.mean(),
    })

# Convertir resultados a DataFrame para fácil visualización
results_df_knn = pd.DataFrame(results)

# Mostrar tabla resumen de resultados por valor de k
print("\nTabla Resumen de Resultados KNN:")
print(results_df_knn)

# Visualizar cómo cambia el rendimiento con k
plt.figure(figsize=(14, 6))

# Plotear Accuracy
plt.subplot(1, 2, 1)
plt.plot(results_df_knn['k'], results_df_knn['Accuracy_TFIDF'], marker='o', label='Accuracy TF-IDF')
plt.plot(results_df_knn['k'], results_df_knn['Accuracy_Embeddings'], marker='o', label='Accuracy Embeddings')
plt.xlabel('Valor de k (Vecinos)')
plt.ylabel('Accuracy')
plt.title('Accuracy vs k para KNN')
plt.xticks(k_values)
plt.grid(True)
plt.legend()

# Plotear F1 Score (o puedes elegir otra métrica clave)
plt.subplot(1, 2, 2)
plt.plot(results_df_knn['k'], results_df_knn['F1_TFIDF'], marker='o', label='F1 Score TF-IDF')
plt.plot(results_df_knn['k'], results_df_knn['F1_Embeddings'], marker='o', label='F1 Score Embeddings')
plt.xlabel('Valor de k (Vecinos)')
plt.ylabel('F1 Score')
plt.title('F1 Score vs k para KNN')
plt.xticks(k_values)
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# Opcional: Visualizar Validación Cruzada
plt.figure(figsize=(10, 6))
plt.plot(results_df_knn['k'], results_df_knn['CV_Accuracy_TFIDF'], marker='o', label='CV Accuracy TF-IDF')
plt.plot(results_df_knn['k'], results_df_knn['CV_Accuracy_Embeddings'], marker='o', label='CV Accuracy Embeddings')
plt.xlabel('Valor de k (Vecinos)')
plt.ylabel('Accuracy Promedio (Validación Cruzada)')
plt.title('Validación Cruzada Accuracy vs k para KNN')
plt.xticks(k_values)
plt.grid(True)
plt.legend()
plt.show()

"""naive bayes"""

|# prompt: GENERA ESTE CODIGO Naive Bayes: Evaluar su desempeño considerando su capacidad para modelar
# distribuciones probabilísticas, especialmente en textos con alta dimensionalidad y que tenga estas metricas de rendimiento: precision,recall,f1score,ROC-ACU, MATRYZ DE CONFUSION Y VALIDACION CRUZADA

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import LabelBinarizer
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Asegurarse de que las variables x (TF-IDF) y y (etiquetas) están disponibles.
# x es la matriz TF-IDF generada previamente (X_tfidf)
# y son las etiquetas de emociones (data_respuestas_con_sentimientos_emb['Emocion'] o similar)

# Dividir los datos en conjuntos de entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Convertir las etiquetas a formato binario para ROC-AUC (para clasificación multiclase)
lb = LabelBinarizer()
y_train_bin = lb.fit_transform(y_train)
y_test_bin = lb.transform(y_test)

print("\nEntrenando y evaluando Naive Bayes:")

# 1. Inicializar y entrenar el modelo Naive Bayes (MultinomialNB es adecuado para conteos/TF-IDF)
# Si estuvieras usando embeddings, podrías considerar GaussianNB o ComplementNB
clf_nb = MultinomialNB()
clf_nb.fit(x_train, y_train)

# 2. Predecir en el conjunto de prueba
y_pred_nb = clf_nb.predict(x_test)

# 3. Calcular métricas de rendimiento
accuracy_nb = clf_nb.score(x_test, y_test)
precision_nb = precision_score(y_test, y_pred_nb, average='weighted') # 'weighted' para multiclase
recall_nb = recall_score(y_test, y_pred_nb, average='weighted')
f1_nb = f1_score(y_test, y_pred_nb, average='weighted')

print(f"--- Métricas en conjunto de prueba ---")
print(f"Precisión (Accuracy): {accuracy_nb:.4f}")
print(f"Precisión: {precision_nb:.4f}")
print(f"Recall: {recall_nb:.4f}")
print(f"F1 Score: {f1_nb:.4f}")

# 4. Calcular ROC-AUC (requiere probabilidades)
try:
    # Obtener las probabilidades predichas
    y_proba_nb = clf_nb.predict_proba(x_test)

    # Calcular ROC-AUC (usando el formato binario de las etiquetas)
    # average='weighted' es común para multiclase, o puedes usar 'macro', 'micro', etc.
    roc_auc_nb = roc_auc_score(y_test_bin, y_proba_nb, average='weighted')
    print(f"ROC-AUC: {roc_auc_nb:.4f}")
except Exception as e:
    print(f"No se pudo calcular ROC-AUC: {e}") # Puede fallar si hay clases con 0 instancias en test o por otros motivos

# 5. Matriz de Confusión
print("\n--- Matriz de Confusión ---")
cm_nb = confusion_matrix(y_test, y_pred_nb)

# Visualizar la matriz de confusión
plt.figure(figsize=(10, 7))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues',
            xticklabels=lb.classes_, yticklabels=lb.classes_) # Usar las clases del LabelBinarizer
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Naive Bayes')
plt.show()

# 6. Validación Cruzada
print("\n--- Validación Cruzada (K-Fold) ---")
kf = KFold(n_splits=10, shuffle=True, random_state=42) # Usar 10 splits
cv_scores_nb = cross_val_score(clf_nb, x, y, cv=kf, scoring='accuracy') # Usar datos completos para CV

print(f"Validación Cruzada (Accuracy) en 10 folds: {cv_scores_nb.mean():.4f} (+/- {cv_scores_nb.std() * 2:.4f})")

# Opcional: Puedes calcular otras métricas en validación cruzada si es necesario
# cv_precision = cross_val_score(clf_nb, x, y, cv=kf, scoring='precision_weighted').mean()
# cv_recall = cross_val_score(clf_nb, x, y, cv=kf, scoring='recall_weighted').mean()
# cv_f1 = cross_val_score(clf_nb, x, y, cv=kf, scoring='f1_weighted').mean()
# print(f"Validación Cruzada (Precision): {cv_precision:.4f}")
# print(f"Validación Cruzada (Recall): {cv_recall:.4f}")
# print(f"Validación Cruzada (F1 Score): {cv_f1:.4f}")

# Resumen de los resultados de Naive Bayes
print("\n--- Resumen Naive Bayes ---")
print(f"Modelo: Naive Bayes (MultinomialNB)")
print(f"Métricas en Test:")
print(f" - Accuracy: {accuracy_nb:.4f}")
print(f" - Precision: {precision_nb:.4f}")
print(f" - Recall: {recall_nb:.4f}")
print(f" - F1 Score: {f1_nb:.4f}")
print(f" - ROC-AUC: {roc_auc_nb:.4f}" if isinstance(roc_auc_nb, float) else f" - ROC-AUC: {roc_auc_nb}")
print(f"Validación Cruzada (10 folds):")
print(f" - Accuracy Promedio: {cv_scores_nb.mean():.4f} (+/- {cv_scores_nb.std() * 2:.4f})")

"""ARBOLES DE DECISION"""

# prompt: GENERA ESTE CODIGO Árboles de Decisión (ID3 y C4.5): Analizar su aplicabilidad como modelos
# interpretables, comprendiendo el flujo de decisión en función de atributos textuales
# clave. y que tenga estas metricas de rendimiento: precision,recall,f1score,ROC-ACU, MATRYZ DE CONFUSION Y VALIDACION CRUZADA

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import LabelBinarizer
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# Asegurarse de que las variables x (TF-IDF), y (etiquetas), x_emb_train y x_emb_test (embeddings)
# están disponibles en este punto del código.
# x es la matriz TF-IDF (X_tfidf)
# y son las etiquetas de emociones (data_respuestas_con_sentimientos_emb['Emocion'] o similar)
# matriz contiene los embeddings generados previamente

# Convertir las etiquetas a formato binario para ROC-AUC (para clasificación multiclase)
# Se necesita un enfoque One-vs-Rest o similar para calcular ROC-AUC en multiclase.
# LabelBinarizer ayuda a crear la representación binaria de las etiquetas.
lb = LabelBinarizer()
y_train_bin = lb.fit_transform(y_train)
y_test_bin = lb.transform(y_test)

print("\nEntrenando y evaluando Árboles de Decisión (ID3/C4.5 - implementado por scikit-learn como CART):")

# 1. Inicializar y entrenar el modelo Árbol de Decisión
# scikit-learn's DecisionTreeClassifier implements the CART algorithm,
# which is a successor to ID3/C4.5 and handles both categorical and numerical data.
# You can influence its behavior towards C4.5 principles by handling categorical features appropriately
# before feeding them to the model (e.g., using one-hot encoding if not TF-IDF/embeddings).
# For this text classification task with TF-IDF or embeddings, CART is suitable.
# max_depth can be adjusted to control complexity and interpretability.
clf_tree = DecisionTreeClassifier(max_depth=10, random_state=42) # Ajusta max_depth si es necesario para interpretacion

# --- Entrenamiento y Evaluación en Datos de Prueba (TF-IDF) ---
print("--- Usando TF-IDF ---")
clf_tree_tfidf = DecisionTreeClassifier(max_depth=10, random_state=42)
clf_tree_tfidf.fit(x_train, y_train)
y_pred_tree_tfidf = clf_tree_tfidf.predict(x_test)

# Métricas de rendimiento para TF-IDF
accuracy_tree_tfidf = clf_tree_tfidf.score(x_test, y_test)
precision_tree_tfidf = precision_score(y_test, y_pred_tree_tfidf, average='weighted')
recall_tree_tfidf = recall_score(y_test, y_pred_tree_tfidf, average='weighted')
f1_tree_tfidf = f1_score(y_test, y_pred_tree_tfidf, average='weighted')

# ROC-AUC (requiere probabilidades y formato binario)
try:
    y_proba_tree_tfidf = clf_tree_tfidf.predict_proba(x_test)
    roc_auc_tree_tfidf = roc_auc_score(y_test_bin, y_proba_tree_tfidf, average='weighted')
except Exception as e:
    roc_auc_tree_tfidf = "N/A"
    print(f"No se pudo calcular ROC-AUC para Árbol de Decisión (TF-IDF): {e}")

print(f"Precisión (Accuracy) TF-IDF: {accuracy_tree_tfidf:.4f}")
print(f"Precisión TF-IDF: {precision_tree_tfidf:.4f}")
print(f"Recall TF-IDF: {recall_tree_tfidf:.4f}")
print(f"F1 Score TF-IDF: {f1_tree_tfidf:.4f}")
print(f"ROC-AUC TF-IDF: {roc_auc_tree_tfidf}")

# Matriz de Confusión para TF-IDF
print("\n--- Matriz de Confusión (TF-IDF) ---")
cm_tree_tfidf = confusion_matrix(y_test, y_pred_tree_tfidf)

# Visualizar la matriz de confusión (TF-IDF)
plt.figure(figsize=(10, 7))
sns.heatmap(cm_tree_tfidf, annot=True, fmt='d', cmap='Blues',
            xticklabels=lb.classes_, yticklabels=lb.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Decision Tree (TF-IDF)')
plt.show()

# Validación Cruzada para TF-IDF
print("\n--- Validación Cruzada (TF-IDF, K-Fold) ---")
kf = KFold(n_splits=10, shuffle=True, random_state=42)
cv_scores_tree_tfidf = cross_val_score(clf_tree_tfidf, x, y, cv=kf, scoring='accuracy') # Usar datos completos para CV

print(f"Validación Cruzada (Accuracy) TF-IDF en 10 folds: {cv_scores_tree_tfidf.mean():.4f} (+/- {cv_scores_tree_tfidf.std() * 2:.4f})")

# --- Entrenamiento y Evaluación en Datos de Prueba (Embeddings - BERT) ---
print("\n--- Usando Embeddings (BERT) ---")
clf_tree_emb = DecisionTreeClassifier(max_depth=10, random_state=42)
# Asegurarse de que x_emb_train y y_train existen
clf_tree_emb.fit(x_emb_train, y_train)
# Asegurarse de que x_emb_test existe
y_pred_tree_emb = clf_tree_emb.predict(x_emb_test)

# Métricas de rendimiento para Embeddings
accuracy_tree_emb = clf_tree_emb.score(x_emb_test, y_test)
precision_tree_emb = precision_score(y_test, y_pred_tree_emb, average='weighted')
recall_tree_emb = recall_score(y_test, y_pred_tree_emb, average='weighted')
f1_tree_emb = f1_score(y_test, y_pred_tree_emb, average='weighted')

# ROC-AUC (requiere probabilidades y formato binario)
try:
    y_proba_tree_emb = clf_tree_emb.predict_proba(x_emb_test)
    roc_auc_tree_emb = roc_auc_score(y_test_bin, y_proba_tree_emb, average='weighted')
except Exception as e:
    roc_auc_tree_emb = "N/A"
    print(f"No se pudo calcular ROC-AUC para Árbol de Decisión (Embeddings): {e}")


print(f"Precisión (Accuracy) Embeddings: {accuracy_tree_emb:.4f}")
print(f"Precisión Embeddings: {precision_tree_emb:.4f}")
print(f"Recall Embeddings: {recall_tree_emb:.4f}")
print(f"F1 Score Embeddings: {f1_tree_emb:.4f}")
print(f"ROC-AUC Embeddings: {roc_auc_tree_emb}")

# Matriz de Confusión para Embeddings
print("\n--- Matriz de Confusión (Embeddings) ---")
cm_tree_emb = confusion_matrix(y_test, y_pred_tree_emb)

# Visualizar la matriz de confusión (Embeddings)
plt.figure(figsize=(10, 7))
sns.heatmap(cm_tree_emb, annot=True, fmt='d', cmap='Blues',
            xticklabels=lb.classes_, yticklabels=lb.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Decision Tree (Embeddings)')
plt.show()


# Validación Cruzada para Embeddings
print("\n--- Validación Cruzada (Embeddings, K-Fold) ---")
# Usar los embeddings completos (matriz) y etiquetas completas (y) para CV
cv_scores_tree_emb = cross_val_score(clf_tree_emb, matriz, y, cv=kf, scoring='accuracy')
print(f"Validación Cruzada (Accuracy) Embeddings en 10 folds: {cv_scores_tree_emb.mean():.4f} (+/- {cv_scores_tree_emb.std() * 2:.4f})")

# Resumen de los resultados de Árboles de Decisión
print("\n--- Resumen Árboles de Decisión ---")
print(f"Modelo: Árbol de Decisión (CART)")
print(f"Métricas en Test (TF-IDF):")
print(f" - Accuracy: {accuracy_tree_tfidf:.4f}")
print(f" - Precision: {precision_tree_tfidf:.4f}")
print(f" - Recall: {recall_tree_tfidf:.4f}")
print(f" - F1 Score: {f1_tree_tfidf:.4f}")
print(f" - ROC-AUC: {roc_auc_tree_tfidf:.4f}" if isinstance(roc_auc_tree_tfidf, float) else f" - ROC-AUC: {roc_auc_tree_tfidf}")
print(f"Validación Cruzada (TF-IDF, 10 folds):")
print(f" - Accuracy Promedio: {cv_scores_tree_tfidf.mean():.4f} (+/- {cv_scores_tree_tfidf.std() * 2:.4f})")

print(f"\nMétricas en Test (Embeddings):")
print(f" - Accuracy: {accuracy_tree_emb:.4f}")
print(f" - Precision: {precision_tree_emb:.4f}")
print(f" - Recall: {recall_tree_emb:.4f}")
print(f" - F1 Score: {f1_tree_emb:.4f}")
print(f" - ROC-AUC: {roc_auc_tree_emb:.4f}" if isinstance(roc_auc_tree_emb, float) else f" - ROC-AUC: {roc_auc_tree_emb}")
print(f"Validación Cruzada (Embeddings, 10 folds):")
print(f" - Accuracy Promedio: {cv_scores_tree_emb.mean():.4f} (+/- {cv_scores_tree_emb.std() * 2:.4f})")

"""SVM"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import LabelBinarizer

# Asegúrate de tener definidas las siguientes variables:
# x_train, x_test, y_train, y_test: Datos de entrenamiento y prueba para TF-IDF
# x_emb_train, x_emb_test: Datos de entrenamiento y prueba para embeddings
# y: Etiquetas completas
# matriz: Embeddings completos

# Binarizar etiquetas para ROC-AUC
lb = LabelBinarizer()
lb.fit(y)
y_train_bin = lb.transform(y_train)
y_test_bin = lb.transform(y_test)

def entrenar_y_evaluar_svm(X_train, X_test, y_train, y_test, X_full, y_full, modelo_nombre, kernel, param_dist):
    print(f"\n--- Evaluando SVM con {modelo_nombre} y kernel '{kernel}' ---")

    # Búsqueda aleatoria de hiperparámetros
    search = RandomizedSearchCV(
        SVC(kernel=kernel, probability=True, random_state=42),
        param_distributions=param_dist,
        n_iter=5,
        cv=5,
        scoring='accuracy',
        n_jobs=-1,
        random_state=42
    )
    search.fit(X_train, y_train)
    clf_svm = search.best_estimator_
    print(f"Mejores parámetros encontrados: {search.best_params_}")
    print(f"Mejor precisión en validación: {search.best_score_:.4f}")

    # Métricas en conjunto de prueba
    y_pred = clf_svm.predict(X_test)
    accuracy = clf_svm.score(X_test, y_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    print(f"Precisión (Accuracy): {accuracy:.4f}")
    print(f"Precisión: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # ROC-AUC
    try:
        y_proba = clf_svm.predict_proba(X_test)
        y_test_bin_subset = lb.transform(y_test)
        roc_auc = roc_auc_score(y_test_bin_subset, y_proba, average='weighted')
        print(f"ROC-AUC: {roc_auc:.4f}")
    except Exception as e:
        roc_auc = np.nan
        print(f"No se pudo calcular ROC-AUC: {e}")

    # Matriz de Confusión
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=lb.classes_, yticklabels=lb.classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Matriz de Confusión - SVM ({modelo_nombre}, Kernel: {kernel})')
    plt.show()

    # Validación Cruzada
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(clf_svm, X_full, y_full, cv=kf, scoring='accuracy', n_jobs=-1)
    print(f"Validación Cruzada (Accuracy) en 5 folds: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    # Guardar modelo
    joblib.dump(clf_svm, f'svm_{modelo_nombre}_{kernel}.joblib')

    return {
        'modelo': modelo_nombre,
        'kernel': kernel,
        'accuracy_test': accuracy,
        'precision_test': precision,
        'recall_test': recall,
        'f1_test': f1,
        'roc_auc_test': roc_auc,
        'cv_accuracy': cv_scores.mean()
    }

# Definir distribuciones de hiperparámetros
param_dist_linear = {'C': [0.1, 1, 10]}
param_dist_rbf = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.01, 0.1]}

all_svm_results = []

# Evaluar SVM con diferentes kernels (usando TF-IDF)
print("\n--- Evaluación de SVM con TF-IDF ---")
all_svm_results.append(entrenar_y_evaluar_svm(x_train, x_test, y_train, y_test, x, y, "TF-IDF", 'linear', param_dist=param_dist_linear))
all_svm_results.append(entrenar_y_evaluar_svm(x_train, x_test, y_train, y_test, x, y, "TF-IDF", 'rbf', param_dist=param_dist_rbf))

# Evaluar SVM con diferentes kernels (usando Embeddings)
print("\n--- Evaluación de SVM con Embeddings (BERT) ---")
all_svm_results.append(entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, matriz, y, "Embeddings (BERT)", 'linear', param_dist=param_dist_linear))
all_svm_results.append(entrenar_y_evaluar_svm(x_emb_train, x_emb_test, y_train, y_test, matriz, y, "Embeddings (BERT)", 'rbf', param_dist=param_dist_rbf))

# Convertir resultados a DataFrame
svm_results_df = pd.DataFrame(all_svm_results)

# Visualización de resultados
plt.figure(figsize=(18, 8))

# Accuracy en Test
plt.subplot(1, 3, 1)
sns.barplot(data=svm_results_df, x='kernel', y='accuracy_test', hue='modelo', palette='viridis')
plt.title('Accuracy en Test por Kernel y Representación')
plt.ylabel('Accuracy')
plt.xlabel('Kernel SVM')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Representación', loc='upper left')
plt.ylim(0, 1)

# F1 Score en Test
plt.subplot(1, 3, 2)
sns.barplot(data=svm_results_df, x='kernel', y='f1_test', hue='modelo', palette='viridis')
plt.title('F1 Score en Test por Kernel y Representación')
plt.ylabel('F1 Score')
plt.xlabel('Kernel SVM')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Representación', loc='upper left')
plt.ylim(0, 1)

# Accuracy de Validación Cruzada
plt.subplot(1, 3, 3)
sns.barplot(data=svm_results_df, x='kernel', y='cv_accuracy', hue='modelo', palette='viridis')
plt.title('Accuracy Promedio (CV) por Kernel y Representación')
plt.ylabel('Accuracy CV')
plt.xlabel('Kernel SVM')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Representación', loc='upper left')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

# Visualización de ROC-AUC si está disponible
if not svm_results_df['roc_auc_test'].isnull().all():
    plt.figure(figsize=(12, 6))
    sns.barplot(data=svm_results_df.dropna(subset=['roc_auc_test']), x='kernel', y='roc_auc_test', hue='modelo', palette='viridis')
    plt.title('ROC-AUC en Test por Kernel y Representación')
    plt.ylabel('ROC-AUC')
    plt.xlabel('Kernel SVM')
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Representación', loc='upper left')
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

"""random forest"""

# prompt: generame este codigo Random Forest: Incorporar este método de ensamblado basado en múltiples
# árboles de decisión, con el objetivo de mejorar la precisión y robustez del modelo,
# reducir el sobreajuste y evaluar su capacidad predictiva en comparación con
# modelos individuales y que tenga estas metricas de rendimiento: precision,recall,f1score,ROC-ACU, MATRYZ DE CONFUSION Y VALIDACION CRUZADA

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer

print("\nEntrenando y evaluando Random Forest:")

# Función para entrenar y evaluar Random Forest con métricas completas
def entrenar_y_evaluar_rf(X_train, X_test, y_train, y_test, X_full, y_full, modelo_nombre, param_grid=None):
    print(f"\n--- Evaluando Random Forest con {modelo_nombre} ---")

    if param_grid:
        # Ajuste de hiperparámetros con GridSearchCV
        print(f"Realizando ajuste de hiperparámetros con GridSearchCV...")
        # Usar RandomForestClassifier en GridSearchCV
        grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        clf_rf = grid_search.best_estimator_
        print(f"Mejores parámetros encontrados: {grid_search.best_params_}")
        print(f"Mejor precisión en validación: {grid_search.best_score_:.4f}")
    else:
        # Entrenar Random Forest sin ajuste de hiperparámetros
        clf_rf = RandomForestClassifier(n_estimators=100, random_state=42) # n_estimators default is 100
        print("Entrenando modelo sin ajuste de hiperparámetros.")
        clf_rf.fit(X_train, y_train)

    # --- Métricas en conjunto de prueba ---
    print("\n--- Métricas en conjunto de prueba ---")
    y_pred = clf_rf.predict(X_test)

    accuracy = clf_rf.score(X_test, y_test)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    print(f"Precisión (Accuracy): {accuracy:.4f}")
    print(f"Precisión: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # ROC-AUC (requiere probabilidades y formato binario)
    try:
        y_proba = clf_rf.predict_proba(X_test)
        # Usar el LabelBinarizer previamente ajustado en los datos de entrenamiento + test
        y_test_bin_subset = lb.transform(y_test) # Transformar solo las etiquetas del conjunto de prueba
        roc_auc = roc_auc_score(y_test_bin_subset, y_proba, average='weighted')
        print(f"ROC-AUC: {roc_auc:.4f}")
    except Exception as e:
        roc_auc = "N/A"
        print(f"No se pudo calcular ROC-AUC: {e}")


    # --- Matriz de Confusión ---
    print("\n--- Matriz de Confusión ---")
    cm = confusion_matrix(y_test, y_pred)

    # Visualizar la matriz de confusión
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=lb.classes_, yticklabels=lb.classes_) # Usar las clases del LabelBinarizer
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - Random Forest ({modelo_nombre})')
    plt.show()

    # --- Validación Cruzada ---
    print("\n--- Validación Cruzada (K-Fold) ---")
    kf = KFold(n_splits=10, shuffle=True, random_state=42) # Usar 10 splits
    # Usar los datos completos (X_full, y_full) para validación cruzada
    cv_scores = cross_val_score(clf_rf, X_full, y_full, cv=kf, scoring='accuracy', n_jobs=-1)

    print(f"Validación Cruzada (Accuracy) en 10 folds: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    # Opcional: Calcular otras métricas en validación cruzada
    # cv_precision = cross_val_score(clf_rf, X_full, y_full, cv=kf, scoring='precision_weighted', n_jobs=-1).mean()
    # cv_recall = cross_val_score(clf_rf, X_full, y_full, cv=kf, scoring='recall_weighted', n_jobs=-1).mean()
    # cv_f1 = cross_val_score(clf_rf, X_full, y_full, cv=kf, scoring='f1_weighted', n_jobs=-1).mean()
    # print(f"Validación Cruzada (Precision): {cv_precision:.4f}")
    # print(f"Validación Cruzada (Recall): {cv_recall:.4f}")
    # print(f"Validación Cruzada (F1 Score): {cv_f1:.4f}")


    return {
        'modelo': modelo_nombre,
        'accuracy_test': accuracy,
        'precision_test': precision,
        'recall_test': recall,
        'f1_test': f1,
        'roc_auc_test': roc_auc,
        'cv_accuracy': cv_scores.mean()
    }

from sklearn.model_selection import GridSearchCV

# Definir rejilla de hiperparámetros para Random Forest
# Puedes ajustar n_estimators, max_depth, min_samples_split, min_samples_leaf, etc.
param_grid_rf = {
    'n_estimators': [50, 100, 200], # Number of trees in the forest
    'max_depth': [None, 10, 20],    # Maximum depth of the trees
    'min_samples_split': [2, 5, 10] # Minimum number of samples required to split an internal node
    # Add more parameters if needed
}

all_rf_results = []

# --- Evaluar Random Forest (usando TF-IDF) ---
print("\n--- Evaluación de Random Forest con TF-IDF ---")
# Usamos x_train/test y x (TF-IDF completo) para CV
all_rf_results.append(entrenar_y_evaluar_rf(x_train, x_test, y_train, y_test, x, y, "TF-IDF", param_grid=param_grid_rf))

# --- Evaluar Random Forest (usando Embeddings - BERT) ---
print("\n--- Evaluación de Random Forest con Embeddings (BERT) ---")
# Usamos x_emb_train/test y matriz (embeddings completos) para CV
all_rf_results.append(entrenar_y_evaluar_rf(x_emb_train, x_emb_test, y_train, y_test, matriz, y, "Embeddings (BERT)", param_grid=param_grid_rf))


# Convertir todos los resultados de Random Forest a DataFrame
rf_results_df = pd.DataFrame(all_rf_results)

print("\nTabla Resumen de Resultados Random Forest:")
print(rf_results_df)

# Opcional: Visualizar comparación de métricas
plt.figure(figsize=(14, 7))

# Graficar Accuracy en Test
plt.subplot(1, 3, 1)
sns.barplot(data=rf_results_df, x='modelo', y='accuracy_test', palette='viridis')
plt.title('Accuracy en Test para Random Forest')
plt.ylabel('Accuracy')
plt.xlabel('Representación')
plt.ylim(0, 1)

# Graficar F1 Score en Test
plt.subplot(1, 3, 2)
sns.barplot(data=rf_results_df, x='modelo', y='f1_test', palette='viridis')
plt.title('F1 Score en Test para Random Forest')
plt.ylabel('F1 Score')
plt.xlabel('Representación')
plt.ylim(0, 1)

# Graficar Accuracy de Validación Cruzada
plt.subplot(1, 3, 3)
sns.barplot(data=rf_results_df, x='modelo', y='cv_accuracy', palette='viridis')
plt.title('Accuracy Promedio (CV) para Random Forest')
plt.ylabel('Accuracy CV')
plt.xlabel('Representación')
plt.ylim(0, 1)


plt.tight_layout()
plt.show()

# Comparación específica de ROC-AUC si se calculó para todos
if not rf_results_df['roc_auc_test'].isnull().all():
    plt.figure(figsize=(8, 6))
    sns.barplot(data=rf_results_df[rf_results_df['roc_auc_test'] != 'N/A'], x='modelo', y='roc_auc_test', palette='viridis')
    plt.title('ROC-AUC en Test para Random Forest')
    plt.ylabel('ROC-AUC')
    plt.xlabel('Representación')
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

"""grafica de comparacion de algoritmos clasicos"""

# prompt: quiero que me des una grafica donde se comparen los modelos que apenas ejecute los cuales son knn,naive bayes, arboles de decision,svm y random forest

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import joblib

# Asegúrate de que estas variables están disponibles desde tu código anterior
# x: Matriz TF-IDF completa
# y: Etiquetas de emociones completas (como una Serie de pandas)
# matriz: Embeddings completos (Numpy array)
# x_train, x_test, y_train, y_test: Split de datos TF-IDF
# x_emb_train, x_emb_test: Split de datos de Embeddings

# Binarizar etiquetas para ROC-AUC y Matriz de Confusión
lb = LabelBinarizer()
# Ajustar LabelBinarizer a TODAS las posibles clases en 'y'
lb.fit(y)
y_train_bin = lb.transform(y_train)
y_test_bin = lb.transform(y_test)

# Lista para almacenar los resultados de los modelos
comparative_results = []

# --- Recopilar Resultados de los Modelos Ya Ejecutados ---

# KNN (elegir un valor de k que te parezca el mejor de tu análisis anterior, por ejemplo k=5)
# O mejor, usar el resultado promedio de la validación cruzada si es representativo
# Basado en tu análisis anterior, toma el mejor rendimiento (ej. CV Accuracy)
best_k_tfidf_row = results_df_knn.loc[results_df_knn['CV_Accuracy_TFIDF'].idxmax()]
best_k_emb_row = results_df_knn.loc[results_df_knn['CV_Accuracy_Embeddings'].idxmax()]

comparative_results.append({
    'Modelo': f'KNN (k={int(best_k_tfidf_row["k"])})',
    'Representacion': 'TF-IDF',
    'Accuracy_Test': best_k_tfidf_row['Accuracy_TFIDF'],
    'F1_Test': best_k_tfidf_row['F1_TFIDF'],
    'CV_Accuracy': best_k_tfidf_row['CV_Accuracy_TFIDF'],
    'ROC_AUC_Test': best_k_tfidf_row['ROC_AUC_TFIDF'] if isinstance(best_k_tfidf_row['ROC_AUC_TFIDF'], float) else None # Handle 'N/A'
})
comparative_results.append({
    'Modelo': f'KNN (k={int(best_k_emb_row["k"])})',
    'Representacion': 'Embeddings (BERT)',
    'Accuracy_Test': best_k_emb_row['Accuracy_Embeddings'],
    'F1_Test': best_k_emb_row['F1_Embeddings'],
    'CV_Accuracy': best_k_emb_row['CV_Accuracy_Embeddings'],
    'ROC_AUC_Test': best_k_emb_row['ROC_AUC_Embeddings'] if isinstance(best_k_emb_row['ROC_AUC_Embeddings'], float) else None # Handle 'N/A'
})


# Naive Bayes (solo TF-IDF)
comparative_results.append({
    'Modelo': 'Naive Bayes',
    'Representacion': 'TF-IDF',
    'Accuracy_Test': accuracy_nb, # Usando la variable guardada de tu análisis
    'F1_Test': f1_nb,
    'CV_Accuracy': cv_scores_nb.mean(),
    'ROC_AUC_Test': roc_auc_nb if isinstance(roc_auc_nb, float) else None
})

# Árbol de Decisión (TF-IDF y Embeddings)
comparative_results.append({
    'Modelo': 'Árbol de Decisión',
    'Representacion': 'TF-IDF',
    'Accuracy_Test': accuracy_tree_tfidf, # Usando la variable guardada de tu análisis
    'F1_Test': f1_tree_tfidf,
    'CV_Accuracy': cv_scores_tree_tfidf.mean(),
     'ROC_AUC_Test': roc_auc_tree_tfidf if isinstance(roc_auc_tree_tfidf, float) else None
})
comparative_results.append({
    'Modelo': 'Árbol de Decisión',
    'Representacion': 'Embeddings (BERT)',
    'Accuracy_Test': accuracy_tree_emb, # Usando la variable guardada de tu análisis
    'F1_Test': f1_tree_emb,
    'CV_Accuracy': cv_scores_tree_emb.mean(),
    'ROC_AUC_Test': roc_auc_tree_emb if isinstance(roc_auc_tree_emb, float) else None
})

# SVM (Linear y RBF kernels, TF-IDF y Embeddings)
# Basado en tu análisis anterior, toma los resultados de los mejores kernels
# Podrías añadir más kernels o considerar solo los mejores. Aquí añado linear y rbf como ejemplo.
svm_results_linear_tfidf = svm_results_df[(svm_results_df['modelo'] == 'TF-IDF') & (svm_results_df['kernel'] == 'linear')].iloc[0]
svm_results_rbf_tfidf = svm_results_df[(svm_results_df['modelo'] == 'TF-IDF') & (svm_results_df['kernel'] == 'rbf')].iloc[0]
svm_results_linear_emb = svm_results_df[(svm_results_df['modelo'] == 'Embeddings (BERT)') & (svm_results_df['kernel'] == 'linear')].iloc[0]
svm_results_rbf_emb = svm_results_df[(svm_results_df['modelo'] == 'Embeddings (BERT)') & (svm_results_df['kernel'] == 'rbf')].iloc[0]


comparative_results.append({
    'Modelo': 'SVM (Linear)',
    'Representacion': 'TF-IDF',
    'Accuracy_Test': svm_results_linear_tfidf['accuracy_test'],
    'F1_Test': svm_results_linear_tfidf['f1_test'],
    'CV_Accuracy': svm_results_linear_tfidf['cv_accuracy'],
    'ROC_AUC_Test': svm_results_linear_tfidf['roc_auc_test'] if isinstance(svm_results_linear_tfidf['roc_auc_test'], float) else None
})
comparative_results.append({
    'Modelo': 'SVM (RBF)',
    'Representacion': 'TF-IDF',
    'Accuracy_Test': svm_results_rbf_tfidf['accuracy_test'],
    'F1_Test': svm_results_rbf_tfidf['f1_test'],
    'CV_Accuracy': svm_results_rbf_tfidf['cv_accuracy'],
    'ROC_AUC_Test': svm_results_rbf_tfidf['roc_auc_test'] if isinstance(svm_results_rbf_tfidf['roc_auc_test'], float) else None
})
comparative_results.append({
    'Modelo': 'SVM (Linear)',
    'Representacion': 'Embeddings (BERT)',
    'Accuracy_Test': svm_results_linear_emb['accuracy_test'],
    'F1_Test': svm_results_linear_emb['f1_test'],
    'CV_Accuracy': svm_results_linear_emb['cv_accuracy'],
    'ROC_AUC_Test': svm_results_linear_emb['roc_auc_test'] if isinstance(svm_results_linear_emb['roc_auc_test'], float) else None
})
comparative_results.append({
    'Modelo': 'SVM (RBF)',
    'Representacion': 'Embeddings (BERT)',
    'Accuracy_Test': svm_results_rbf_emb['accuracy_test'],
    'F1_Test': svm_results_rbf_emb['f1_test'],
    'CV_Accuracy': svm_results_rbf_emb['cv_accuracy'],
    'ROC_AUC_Test': svm_results_rbf_emb['roc_auc_test'] if isinstance(svm_results_rbf_emb['roc_auc_test'], float) else None
})


# Random Forest (TF-IDF y Embeddings)
rf_results_tfidf = rf_results_df[rf_results_df['modelo'] == 'TF-IDF'].iloc[0]
rf_results_emb = rf_results_df[rf_results_df['modelo'] == 'Embeddings (BERT)'].iloc[0]

comparative_results.append({
    'Modelo': 'Random Forest',
    'Representacion': 'TF-IDF',
    'Accuracy_Test': rf_results_tfidf['accuracy_test'],
    'F1_Test': rf_results_tfidf['f1_test'],
    'CV_Accuracy': rf_results_tfidf['cv_accuracy'],
    'ROC_AUC_Test': rf_results_tfidf['roc_auc_test'] if isinstance(rf_results_tfidf['roc_auc_test'], float) else None
})
comparative_results.append({
    'Modelo': 'Random Forest',
    'Representacion': 'Embeddings (BERT)',
    'Accuracy_Test': rf_results_emb['accuracy_test'],
    'F1_Test': rf_results_emb['f1_test'],
    'CV_Accuracy': rf_results_emb['cv_accuracy'],
    'ROC_AUC_Test': rf_results_emb['roc_auc_test'] if isinstance(rf_results_emb['roc_auc_test'], float) else None
})

# Convertir la lista de resultados a DataFrame
comparative_df = pd.DataFrame(comparative_results)

# Opcional: Reorganizar las columnas si quieres un orden específico
comparative_df = comparative_df[['Modelo', 'Representacion', 'Accuracy_Test', 'F1_Test', 'CV_Accuracy', 'ROC_AUC_Test']]

print("\n--- Tabla Comparativa de Resultados de Modelos ---")
print(comparative_df)


# --- Visualización de la Gráfica Comparativa ---

# Reordenar el DataFrame para la visualización
# Puedes pivotar el DataFrame para tener los Modelos en las filas y las métricas como valores
# O usar long format para seaborn

# Ejemplo de visualización de Accuracy en Test usando seaborn (long format)
plt.figure(figsize=(14, 7))
sns.barplot(data=comparative_df, x='Modelo', y='Accuracy_Test', hue='Representacion', palette='viridis')
plt.title('Comparación de Accuracy en Test por Modelo y Representación')
plt.ylabel('Accuracy')
plt.xlabel('Modelo')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.legend(title='Representación', loc='upper left')
plt.tight_layout()
plt.show()

# Ejemplo de visualización de F1 Score en Test
plt.figure(figsize=(14, 7))
sns.barplot(data=comparative_df, x='Modelo', y='F1_Test', hue='Representacion', palette='viridis')
plt.title('Comparación de F1 Score en Test por Modelo y Representación')
plt.ylabel('F1 Score')
plt.xlabel('Modelo')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.legend(title='Representación', loc='upper left')
plt.tight_layout()
plt.show()

# Ejemplo de visualización de CV Accuracy
plt.figure(figsize=(14, 7))
sns.barplot(data=comparative_df, x='Modelo', y='CV_Accuracy', hue='Representacion', palette='viridis')
plt.title('Comparación de Accuracy Promedio (CV) por Modelo y Representación')
plt.ylabel('Accuracy Promedio (CV)')
plt.xlabel('Modelo')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.legend(title='Representación', loc='upper left')
plt.tight_layout()
plt.show()

# Ejemplo de visualización de ROC-AUC si está disponible
if not comparative_df['ROC_AUC_Test'].isnull().all():
     plt.figure(figsize=(14, 7))
     # Filtrar filas donde ROC_AUC_Test no es None/NaN antes de graficar
     sns.barplot(data=comparative_df.dropna(subset=['ROC_AUC_Test']), x='Modelo', y='ROC_AUC_Test', hue='Representacion', palette='viridis')
     plt.title('Comparación de ROC-AUC en Test por Modelo y Representación')
     plt.ylabel('ROC-AUC')
     plt.xlabel('Modelo')
     plt.xticks(rotation=45, ha='right')
     plt.ylim(0, 1)
     plt.legend(title='Representación', loc='upper left')
     plt.tight_layout()
     plt.show()

# prompt: quiero que me des una grafica donde se comparen los modelos que apenas ejecute los cuales son knn,naive bayes, arboles de decision,svm y random forest pero solo los que son de TF-IDF

# Filtrar solo los resultados para TF-IDF
comparative_df_tfidf = comparative_df[comparative_df['Representacion'] == 'TF-IDF'].copy()

# Si KNN tiene entradas con diferentes k, selecciona la mejor o una representativa
# Si en comparative_df ya incluiste el "mejor" KNN TF-IDF, no necesitas filtrar por k aquí.
# Si no, asegúrate de seleccionar la fila correcta.
# Ejemplo: si quieres el KNN con el mejor CV Accuracy de TF-IDF:
# best_knn_tfidf_row = comparative_df[
#     (comparative_df['Representacion'] == 'TF-IDF') &
#     (comparative_df['Modelo'].str.contains('KNN'))
# ].loc[lambda df: df['CV_Accuracy'].idxmax()]
# comparative_df_tfidf = pd.concat([comparative_df_tfidf[~comparative_df_tfidf['Modelo'].str.contains('KNN')], best_knn_tfidf_row.to_frame().T], ignore_index=True)

# Asegúrate de que los nombres de los modelos coincidan exactamente con los que quieres incluir
modelos_tfidf = ['KNN (k=5)', 'Naive Bayes', 'Árbol de Decisión', 'SVM (Linear)', 'SVM (RBF)', 'Random Forest'] # Ajusta los nombres de KNN y SVM kernels si son diferentes en tu df

# Filtrar el DataFrame para incluir solo los modelos deseados con representación TF-IDF
comparative_df_tfidf_filtered = comparative_df_tfidf[comparative_df_tfidf['Modelo'].isin(modelos_tfidf)]

# --- Visualización de la Gráfica Comparativa para TF-IDF ---

# Usar el DataFrame filtrado para graficar
plt.figure(figsize=(12, 7))
sns.barplot(data=comparative_df_tfidf_filtered, x='Modelo', y='Accuracy_Test', palette='viridis')
plt.title('Comparación de Accuracy en Test (TF-IDF)')
plt.ylabel('Accuracy')
plt.xlabel('Modelo')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 7))
sns.barplot(data=comparative_df_tfidf_filtered, x='Modelo', y='F1_Test', palette='viridis')
plt.title('Comparación de F1 Score en Test (TF-IDF)')
plt.ylabel('F1 Score')
plt.xlabel('Modelo')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 7))
sns.barplot(data=comparative_df_tfidf_filtered, x='Modelo', y='CV_Accuracy', palette='viridis')
plt.title('Comparación de Accuracy Promedio (Validación Cruzada) (TF-IDF)')
plt.ylabel('Accuracy Promedio (CV)')
plt.xlabel('Modelo')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

# ROC-AUC si está disponible para TF-IDF
if not comparative_df_tfidf_filtered['ROC_AUC_Test'].isnull().all():
     plt.figure(figsize=(12, 7))
     sns.barplot(data=comparative_df_tfidf_filtered.dropna(subset=['ROC_AUC_Test']), x='Modelo', y='ROC_AUC_Test', palette='viridis')
     plt.title('Comparación de ROC-AUC en Test (TF-IDF)')
     plt.ylabel('ROC-AUC')
     plt.xlabel('Modelo')
     plt.xticks(rotation=45, ha='right')
     plt.ylim(0, 1)
     plt.tight_layout()
     plt.show()

# prompt: quiero que me des un codigo donde me des la precision que tiene mi modelo de prediccion

# Ya has calculado la precisión (Accuracy) para varios modelos anteriormente.
# Las variables que contienen estos valores son, por ejemplo:
# accuracy_svm_bert_linear, accuracy_svm_bert_rbf, etc. (para SVM con embeddings)
# accuracy_svm_linear, accuracy_svm_rbf, etc. (para SVM sin embeddings/TF-IDF)
# accuracy_nb (para Naive Bayes)
# accuracy_tree_tfidf, accuracy_tree_emb (para Árbol de Decisión)
# accuracy_rf_tfidf, accuracy_rf_emb (para Random Forest)
# accuracy_tfidf, accuracy_emb (para KNN con un k específico)

# Además de la Accuracy, también has calculado:
# Precision: precision_score(...) -> precision_svm_bert_linear, precision_nb, etc.
# Recall: recall_score(...) -> recall_svm_bert_linear, recall_nb, etc.
# F1 Score: f1_score(...) -> f1_svm_bert_linear, f1_nb, etc.
# ROC-AUC: roc_auc_score(...) -> roc_auc_nb, roc_auc_tree_tfidf, etc.
# Validación Cruzada (Accuracy promedio): cv_scores_svm_bert_linear.mean(), cv_scores_nb.mean(), etc.

# Para presentar la precisión de tu modelo de predicción, puedes simplemente imprimir
# las métricas de rendimiento que ya has calculado para el modelo o modelos que consideres
# que son tu "modelo de predicción" final.

# Por ejemplo, si consideras que el mejor modelo es el SVM con kernel RBF y embeddings:
print("\n--- Precisión del modelo SVM con Embeddings (BERT) y kernel RBF ---")
# Asegúrate de que las variables correspondientes existen y tienen los valores calculados
try:
    print(f"Accuracy en Test: {accuracy_svm_bert_rbf:.4f}")
    print(f"Precisión en Test (Weighted): {precision_svm_bert_rbf:.4f}")
    print(f"Recall en Test (Weighted): {recall_svm_bert_rbf:.4f}")
    print(f"F1 Score en Test (Weighted): {f1_svm_bert_rbf:.4f}")
    # Intenta imprimir ROC-AUC si se calculó para este modelo
    # NOTA: Tu código actual no guarda roc_auc_score para cada combinación SVM kernel/embedding
    # en variables separadas, solo en la función general. Puedes modificar la función
    # `entrenar_y_evaluar_svm` para que devuelva o imprima este valor para el mejor modelo
    # encontrado por GridSearchCV/RandomizedSearchCV.
    # Por ahora, si se calculó y guardó, imprímelo.
    # print(f"ROC-AUC en Test (Weighted): {roc_auc_svm_bert_rbf:.4f}") # <-- Esta variable no existe en el código proporcionado

    # Puedes referenciar los resultados del DataFrame `svm_results_df`
    # para obtener métricas específicas del mejor modelo encontrado.
    best_svm_bert_rbf_row = svm_results_df[
        (svm_results_df['modelo'] == 'Embeddings (BERT)') &
        (svm_results_df['kernel'] == 'rbf')
    ].iloc[0]
    print(f"Accuracy en Test (desde DF): {best_svm_bert_rbf_row['accuracy_test']:.4f}")
    print(f"Precisión en Test (desde DF): {best_svm_bert_rbf_row['precision_test']:.4f}")
    print(f"Recall en Test (desde DF): {best_svm_bert_rbf_row['recall_test']:.4f}")
    print(f"F1 Score en Test (desde DF): {best_svm_bert_rbf_row['f1_test']:.4f}")
    if isinstance(best_svm_bert_rbf_row['roc_auc_test'], float):
         print(f"ROC-AUC en Test (desde DF): {best_svm_bert_rbf_row['roc_auc_test']:.4f}")
    else:
         print(f"ROC-AUC en Test (desde DF): {best_svm_bert_rbf_row['roc_auc_test']}")
    print(f"Accuracy Promedio CV: {best_svm_bert_rbf_row['cv_accuracy']:.4f}")


except NameError:
    print("Las variables de precisión para SVM con Embeddings (BERT) y kernel RBF no están definidas.")
    print("Asegúrate de haber ejecutado las celdas de entrenamiento de SVM.")
    # Puedes usar directamente el DataFrame de resultados si las variables no están disponibles
    # best_svm_bert_rbf_row = svm_results_df[
    #     (svm_results_df['modelo'] == 'Embeddings (BERT)') &
    #     (svm_results_df['kernel'] == 'rbf')
    # ].iloc[0]
    # print(f"Accuracy en Test (desde DF): {best_svm_bert_rbf_row['accuracy_test']:.4f}")
    # ... y otras métricas desde el DataFrame

print("\n--- Comparativa General de Modelos (desde la tabla resumen) ---")
# Puedes imprimir la tabla comparativa final que ya generaste
if 'comparative_df' in locals():
    print(comparative_df)
else:
    print("El DataFrame comparativo (comparative_df) no está disponible.")

# Para obtener la precisión específica de "tu modelo de predicción" final,
# deberías seleccionar cuál de los modelos entrenados es el que vas a usar
# para predicciones futuras y mostrar sus métricas.

# Por ejemplo, si decides que el Random Forest entrenado con Embeddings es tu mejor modelo:
# Asegúrate de que clf_rf es el modelo entrenado con embeddings (último entrenado si no lo guardaste)
# o carga el modelo guardado con joblib.
try:
    # Asumiendo que 'clf_rf' es el último modelo Random Forest entrenado (con Embeddings si fue el último)
    # y que 'x_test' y 'y_test' corresponden al split de Embeddings
    # Si no estás seguro, es mejor guardar y cargar el modelo explícitamente.

    # Recuperar el resultado del Random Forest con Embeddings del DataFrame
    rf_results_emb = rf_results_df[rf_results_df['modelo'] == 'Embeddings (BERT)'].iloc[0]

    print("\n--- Precisión del modelo Random Forest con Embeddings (BERT) (Si este es tu modelo final) ---")
    print(f"Accuracy en Test: {rf_results_emb['accuracy_test']:.4f}")
    print(f"Precisión en Test (Weighted): {rf_results_emb['precision_test']:.4f}")
    print(f"Recall en Test (Weighted): {rf_results_emb['recall_test']:.4f}")
    print(f"F1 Score en Test (Weighted): {rf_results_emb['f1_test']:.4f}")
    if isinstance(rf_results_emb['roc_auc_test'], float):
        print(f"ROC-AUC en Test (Weighted): {rf_results_emb['roc_auc_test']:.4f}")
    else:
        print(f"ROC-AUC en Test (Weighted): {rf_results_emb['roc_auc_test']}")
    print(f"Accuracy Promedio CV: {rf_results_emb['cv_accuracy']:.4f}")

except NameError:
    print("Las variables para Random Forest con Embeddings no están definidas o el DataFrame no está disponible.")


# En resumen:
# Ya has hecho todo el cálculo de precisión y otras métricas (Accuracy, Precision, Recall, F1, ROC-AUC, CV Accuracy).
# La forma de "dar" la precisión de tu modelo es seleccionando el modelo que consideras
# el mejor basándote en esos resultados y presentar sus métricas.
# Las tablas y gráficos que ya generaste (comparative_df, svm_results_df, rf_results_df, results_df_knn)
# resumen el rendimiento de los diferentes modelos y representaciones.
# Simplemente tienes que indicar cuál es tu "modelo de predicción" final y mostrar sus métricas relevantes
# desde esos DataFrames o variables guardadas.

# prompt: quiero que me generes otro codigo donde peuda hacer predicciones pero con con td-idf de naibe bayes

# Naive Bayes con TF-IDF (Basado en el código existente)

# Asegurarse de que las variables x (TF-IDF) y y (etiquetas) están disponibles.
# x es la matriz TF-IDF generada previamente (X_tfidf)
# y son las etiquetas de emociones (data_respuestas_con_sentimientos_emb['Emocion'] o similar)

# Dividir los datos en conjuntos de entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Convertir las etiquetas a formato binario para ROC-AUC (para clasificación multiclase)
lb = LabelBinarizer()
y_train_bin = lb.fit_transform(y_train)
y_test_bin = lb.transform(y_test)

print("\nEntrenando y evaluando Naive Bayes con TF-IDF:")

# 1. Inicializar y entrenar el modelo Naive Bayes (MultinomialNB es adecuado para conteos/TF-IDF)
clf_nb_tfidf = MultinomialNB()
clf_nb_tfidf.fit(x_train, y_train)

# 2. Predecir en el conjunto de prueba
y_pred_nb_tfidf = clf_nb_tfidf.predict(x_test)

# 3. Calcular métricas de rendimiento
accuracy_nb_tfidf = clf_nb_tfidf.score(x_test, y_test)
precision_nb_tfidf = precision_score(y_test, y_pred_nb_tfidf, average='weighted') # 'weighted' para multiclase
recall_nb_tfidf = recall_score(y_test, y_pred_nb_tfidf, average='weighted')
f1_nb_tfidf = f1_score(y_test, y_pred_nb_tfidf, average='weighted')

print(f"--- Métricas en conjunto de prueba (TF-IDF) ---")
print(f"Precisión (Accuracy): {accuracy_nb_tfidf:.4f}")
print(f"Precisión: {precision_nb_tfidf:.4f}")
print(f"Recall: {recall_nb_tfidf:.4f}")
print(f"F1 Score: {f1_nb_tfidf:.4f}")

# 4. Calcular ROC-AUC (requiere probabilidades)
try:
    # Obtener las probabilidades predichas
    y_proba_nb_tfidf = clf_nb_tfidf.predict_proba(x_test)

    # Calcular ROC-AUC (usando el formato binario de las etiquetas)
    roc_auc_nb_tfidf = roc_auc_score(y_test_bin, y_proba_nb_tfidf, average='weighted')
    print(f"ROC-AUC: {roc_auc_nb_tfidf:.4f}")
except Exception as e:
    print(f"No se pudo calcular ROC-AUC para Naive Bayes (TF-IDF): {e}")

# 5. Matriz de Confusión
print("\n--- Matriz de Confusión (TF-IDF) ---")
cm_nb_tfidf = confusion_matrix(y_test, y_pred_nb_tfidf)

# Visualizar la matriz de confusión
plt.figure(figsize=(10, 7))
sns.heatmap(cm_nb_tfidf, annot=True, fmt='d', cmap='Blues',
            xticklabels=lb.classes_, yticklabels=lb.classes_) # Usar las clases del LabelBinarizer
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Naive Bayes (TF-IDF)')
plt.show()

# 6. Validación Cruzada
print("\n--- Validación Cruzada (TF-IDF, K-Fold) ---")
kf = KFold(n_splits=10, shuffle=True, random_state=42) # Usar 10 splits
cv_scores_nb_tfidf = cross_val_score(clf_nb_tfidf, x, y, cv=kf, scoring='accuracy') # Usar datos completos para CV

print(f"Validación Cruzada (Accuracy) TF-IDF en 10 folds: {cv_scores_nb_tfidf.mean():.4f} (+/- {cv_scores_nb_tfidf.std() * 2:.4f})")

# Función para hacer predicciones con el modelo Naive Bayes TF-IDF entrenado
def predecir_emocion_nb_tfidf(texto_ejemplo, vectorizador_tfidf, modelo_nb):
    """
    Realiza una predicción de emoción para un texto dado usando un modelo Naive Bayes
    entrenado con características TF-IDF.

    Args:
        texto_ejemplo (str): El texto de entrada para predecir la emoción.
        vectorizador_tfidf: El objeto TfidfVectorizer ajustado previamente.
        modelo_nb: El objeto MultinomialNB entrenado previamente.

    Returns:
        str: La emoción predicha para el texto de entrada.
    """
    # Preprocesar el texto (asumiendo que tienes la función limpiar_texto y lematizar/tokenizar si es necesario)
    # IMPORTANTE: El preprocesamiento para la predicción debe ser IDÉNTICO al usado para el entrenamiento.
    # Asegúrate de usar las funciones definidas previamente: limpiar_texto, tokenizar, eliminar_stopwords, lematizar
    texto_procesado = limpiar_texto(texto_ejemplo)
    texto_tokenizado = word_tokenize(texto_procesado)
    texto_stopwords_removidas = [word for word in texto_tokenizado if word not in stop_words] # stop_words debe estar definido
    # Asegúrate de que la lematización se aplica correctamente si se usó en el entrenamiento.
    # Si lemmatizar devuelve una lista de listas como en tu código original, podrías necesitar ajustarlo.
    # La función lemmatizar que definiste devuelve una cadena de texto, lo cual es mejor para TF-IDF.
    texto_lematizado = ' '.join([lemmatizar(word) for word in texto_stopwords_removidas])


    # Vectorizar el texto de ejemplo usando el vectorizador TF-IDF entrenado
    # El vectorizador espera una lista de cadenas de texto
    texto_ejemplo_vectorizado = vectorizador_tfidf.transform([texto_lematizado])

    # Realizar la predicción usando el modelo Naive Bayes entrenado
    prediccion = modelo_nb.predict(texto_ejemplo_vectorizado)

    # La predicción ya es la emoción predicha en formato string
    emocion_predicho = prediccion[0]

    return emocion_predicho

# --- Ejemplo de uso del modelo entrenado con Naive Bayes TF-IDF ---

# Usar el vectorizador TF-IDF y el modelo Naive Bayes entrenados previamente
# tfidf_vectorizer y clf_nb_tfidf deben estar disponibles desde la ejecución anterior.

print("\n--- Prueba de predicción con Naive Bayes (TF-IDF) ---")

# Ejemplos de prueba
ejemplos_texto = [
    "Me siento muy feliz hoy.",
    "Que día tan triste.",
    "Esto me da mucho asco, no puedo comerlo.",
    "Estoy muy enojado con la situación.",
    "Sentí mucho miedo en la oscuridad.",
    "Qué sorpresa tan agradable, no me lo esperaba."
]

for texto_ejemplo in ejemplos_texto:
    emocion_predicha = predecir_emocion_nb_tfidf(texto_ejemplo, tfidf_vectorizer, clf_nb_tfidf)
    print(f"Texto: '{texto_ejemplo}' -> Emoción predicha (NB-TFIDF): '{emocion_predicha}'")

# Ejemplo con entrada del usuario
# print("\nIngresa un texto para predecir la emoción:")
# texto_usuario = input()
# emocion_predicha_usuario = predecir_emocion_nb_tfidf(texto_usuario, tfidf_vectorizer, clf_nb_tfidf)
# print(f"Texto ingresado: '{texto_usuario}' -> Emoción predicha (NB-TFIDF): '{emocion_predicha_usuario}'")